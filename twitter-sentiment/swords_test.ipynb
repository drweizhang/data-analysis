import nltk
import re
import random
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk.sentiment.vader as vd


#### SWORDS STARTS ####
additional  = ['rt','rts','retweet'] #we'll store additional stopwords here
standard_swords = set().union(stopwords.words('english'),additional) #big list containing all the stopwords + our additional ones
self_defined_stopwords = []
#### END ####


sid_obj = vd.SentimentIntensityAnalyzer()
sentense = "This movie is not great, but NOT the worst."

csv = pd.read_excel("output/IVF 2019 Mar-April sentiment.xlsx")
swords_file = open("self_defined_swords.txt", "r")
content_list = swords_file.readlines()
self_defined_stopwords = list(map(lambda s: s.strip(), content_list))

for i in csv[['text']]:
    sentenseList = (csv[['text']][i].values)

    

test = random.choices(sentenseList, k=3)

for sentense in test:
    print("The tweet text is: \n")
    print(sentense)
    print("\n")
    # lower case and tokenization

    cleaned_sentense = re.sub('(@[a-z0-9]+)\w+',' ', sentense)
    cleaned_sentense = re.sub('(http\S+)', ' ', cleaned_sentense)
    cleaned_sentense = re.sub('([^0-9a-z \t])',' ', cleaned_sentense)
    cleaned_sentense = re.sub(' +',' ', cleaned_sentense)


    #cleaned_sentense = ''.join(char for char in sentense if char.isalnum())
    print("Tokenized text is: \n")
    text_tokens = word_tokenize(cleaned_sentense)
    print(text_tokens)
    print("\n")


    # remove standard stop words
    print("Tokenized text with STANDARD stopwords removed: \n")
    tokens_without_sw = [word for word in text_tokens if not word in standard_swords]
    print(tokens_without_sw)
    print("\n")

    
    # remove self stop words
    print("Tokenized text with SELF Defined stopwords removed: \n")
    tokens_without_sw_self = [word for word in text_tokens if not word in self_defined_stopwords]
    print(tokens_without_sw_self)
    print("\n")


    a = sid_obj.polarity_scores(" ".join(text_tokens))
    print("Sentiment for tokenized text is: \n")
    for k in sorted(a):
        print('{0}: {1}, '.format(k, a[k]), end='')

    print("\n")

    b = sid_obj.polarity_scores(" ".join(tokens_without_sw))
    print("Sentiment for tokenized text & removing STANDARD stopwords is: \n")
    for k in sorted(b):
        print('{0}: {1}, '.format(k, b[k]), end='')
        
    print("\n")
    
    c = sid_obj.polarity_scores(" ".join(tokens_without_sw_self))
    print("Sentiment for tokenized text & removing SELF Defined stopwords is: \n")
    for k in sorted(c):
        print('{0}: {1}, '.format(k, b[k]), end='')
        
    print("\n")
    
